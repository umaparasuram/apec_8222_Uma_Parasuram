{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This assignment will give you a real (active) research topic that I've discussed a little bit in class: predicting carbon storage as a function of high-resolution gridded data. In the class google drive you will find all the data you need. I added it just recently so if you don't have it, be sure to go get it first. \n",
                "\n",
                "This assignment will have you use the automated variable selection approach within LASSO to deal with a common situation in regressions on raster-stacks: we have so much data everything is significant but will lead to massive overfitting. The basic approach used here will involve reading in 2d rasters, flattening them into a 1d column ready to add to a dataframe shaped object, which we will use as our X matrix.\n",
                "\n",
                "Please turn in the completed Notebook (.ipynb) file that includes the results you generate. \n",
                "\n",
                "Below is some starter code along with specific assignment questions.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Load libraries\n",
                "import numpy as np\n",
                "import os\n",
                "from osgeo import gdal\n",
                "from sklearn.linear_model import Lasso\n",
                "from matplotlib import pyplot as plt\n",
                "from statsmodels.api import OLS"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Download the data and set paths\n",
                "\n",
                "Download the latest data from the class's google drive. In there, you will need the the files in `Data/python_assignment_2` data and assign a relative path to the `soyo_tile` directory in that assignment directory. It is your task to ensure your script runs in the right location and the data is stored in the right location that this relative path works."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 1 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Set raster paths \n",
                "\n",
                "Assign each of the raster paths in the directory to a dictionary for later use. I've included most of the code (so you don't have to waste your time typing), but add in the missing paths."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 2 code\n",
                "\n",
                "raster_paths = {}\n",
                "\n",
                "# First is the dependent varable, Above Ground Carbon (AGB) in tons, measured at 30 meters globally (here it is clipped to a smaller area)\n",
                "raster_paths['agb_observed_baccini_2000_30m'] = os.path.join(data_dir, \"agb_observed_baccini_2000_30m.tif\")\n",
                "\n",
                "# Here are some of the independent variables\n",
                "raster_paths['CRFVOL_M_sl1_250m'] = os.path.join(data_dir, \"CRFVOL_M_sl1_250m.tif\")\n",
                "raster_paths['HISTPR_250m'] = os.path.join(data_dir, \"HISTPR_250m.tif\")\n",
                "raster_paths['OCDENS_M_sl1_250m'] = os.path.join(data_dir, \"OCDENS_M_sl1_250m.tif\")\n",
                "raster_paths['PHIHOX_M_sl1_250m'] = os.path.join(data_dir, \"PHIHOX_M_sl1_250m.tif\")\n",
                "raster_paths['roughness_30s'] = os.path.join(data_dir, \"roughness_30s.tif\")\n",
                "raster_paths['SLGWRB_250m'] = os.path.join(data_dir, \"SLGWRB_250m.tif\")\n",
                "raster_paths['SLTPPT_M_sl1_250m'] = os.path.join(data_dir, \"SLTPPT_M_sl1_250m.tif\")\n",
                "raster_paths['terrain_ruggedness_index_30s'] = os.path.join(data_dir, \"terrain_ruggedness_index_30s.tif\")\n",
                "raster_paths['TEXMHT_M_sl1_250m'] = os.path.join(data_dir, \"TEXMHT_M_sl1_250m.tif\")\n",
                "raster_paths['wc2.0_bio_30s_01'] = os.path.join(data_dir, \"wc2.0_bio_30s_01.tif\")\n",
                "raster_paths['alt_30s'] = os.path.join(data_dir, \"alt_30s.tif\")\n",
                "raster_paths['AWCh1_M_sl1_250m'] = os.path.join(data_dir, \"AWCh1_M_sl1_250m.tif\")\n",
                "raster_paths['BDRICM_M_250m'] = os.path.join(data_dir, \"BDRICM_M_250m.tif\")\n",
                "raster_paths['BDRLOG_M_250m'] = os.path.join(data_dir, \"BDRLOG_M_250m.tif\")\n",
                "raster_paths['BLDFIE_M_sl1_250m'] = os.path.join(data_dir, \"BLDFIE_M_sl1_250m.tif\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Open the rasters\n",
                "\n",
                "Our dependent variable will be 30 meter observations of carbon storage from Baccini et al. (unpublished, but soon to be published) data. The label I assigned in the dictionary above was agb_observed_baccini_2000_30m for this variable. Use gdal.Open, GetRasterBand(1) and ReadAsArray() to read this geotiff as a numpy file\n",
                "\n",
                "Side note: If you get an error like: \"ERROR 4: This is a BigTIFF file.  BigTIFF is not supported by this version of GDAL and libtiff.\" make sure you have installed gdal with the mamba method from lecture 1. "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 3 code\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Define some arrays\n",
                "\n",
                "Create an empty numpy array (or full of zeros) of the right shape to house all our raster data. A very CPU-efficient way of arranging a stack of 2d rasters (which would be 3d once stacked up), is to flatten each 2d raster into a longer 1d array. This will go into our X matrix. In order to create the right sized X matrix, first get the n_obs and n_vars by inspecting the dependent variable raster and the dictionary of inputs above. Note that the n_vars should be the number of independent AND dependent variables."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 4 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Load all the independent variables\n",
                "\n",
                "- Iterate through the dictionary and load each raster as a 2d array\n",
                "- flatten it to 1d using the .flatten() method in numpy\n",
                "- Assign this 1d array to the correct column of the data array. By convention, the depvar will be the first column.\n",
                "\n",
                "Hint, assuming you have arranged your X array in the correct way, it should have observations (pixels) as rows and variables as cols. Given that each flattened array is for one variable and is as long as there are rows, a convenient way of assigning it would be to use numpy slice notation, potentially similar to:\n",
                "\n",
                "`data_array[:, column_index_integer]`\n",
                "\n",
                "The first colon just denotes the whole row and the column index is an integer you could create pointing to the right row.\n",
                "\n",
                "Some incomplete code to get you started is below."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 5 code\n",
                "\n",
                "for name, path in raster_paths.items():\n",
                "    print('Loading', path)\n",
                "    'MISSING STUFF'\n",
                "    flattened_raster_array = band.ReadAsArray().flatten()\n",
                "    data_array[:, col_index] = flattened_raster_array\n",
                "    feature_names.append(name)\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 6: \n",
                "\n",
                "Extract the first array row of the data_array and assign it to y. Assign the rest to X."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 6 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 7:\n",
                "\n",
                "Split the X and y into testing and training data such that the training data is the first million pixels and the testing data is the next 200,000. Do this using numpy slice notation on the X and y variables you created."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 7 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 8 (optional but useful):\n",
                "\n",
                "To make the code run faster, we are going to use every 10th pixel. We can easily get this via numpy slicing again, using x_train[::10] to get every 10th pixel."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 8 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 9:\n",
                "\n",
                "Create a Lasso object (using the default penalty term alpha) and fit it to the training data. Create and print out a vector of predicted carbon values. Also print out the score using the lasso object's .score() method on the TESTING data. Print out the fitted lasso score."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 9 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 10, optional and just for fun:.\n",
                "\n",
                "To view how our projections LOOK, we can create a predicted matrix on the whole X, reshape it back into the original 2d shape and look at it. You can compare this to the input array to visualize how it looks. Note that this will only work if you name your objects like mine."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 10 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 11:\n",
                "\n",
                "Create a list of 30 alphas using ` np.logspace(-1, 3, 30)`. \n",
                "\n",
                "Using a for loop iterate over those alphas and run the Lasso model like above, but using the alpha values in the loop. Print out the fit score at each step. Using matplotlib, plot how this value changes as alpha changes. Finally, extract the best alpha of the bunch. "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 11 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 12: \n",
                "\n",
                "Rerun the lasso with that best value and identify all of the coefficiencts that were \"selected\" ie had non-zero values. Save these coefficient indices and labels to a list."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 12 code"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 13:\n",
                "\n",
                "Using Statsmodels, run an OLS version on the selected variables.\n",
                "\n",
                "Use print to show the results table.\n",
                "\n",
                "Write a description of any advantages this approach has over vanilla OLS.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Step 13 code"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": [
        {
            "kernelspec": {
                "name": "python3",
                "language": "python",
                "display_name": "Python 3 (ipykernel)"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 4
}